{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Statistics\n",
    "\n",
    "\n",
    "Another way of approaching statistical inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting Started: An Example\n",
    "\n",
    "Let's regress income on years of schooling, from generated data:\n",
    "\n",
    "$$ income = 100 + 20 * school.years + \\epsilon$$\n",
    "\n",
    "<img src=\"figures/example_actuals.png\" width=\"300\">\n",
    "\n",
    "We run this (very realistic) data through:\n",
    "1. OLS - ala frequentist method (what we all know as OLS)\n",
    "2. OLS - ala Bayesian method with flat prior\n",
    "3. OLS - ala Bayesian method with (wrongly) informed prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Estimation: frequentist\n",
    "\n",
    "<img src=\"figures/example_frequentist.png\" width=\"450\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Estimation: Bayesian with flat prior\n",
    "\n",
    "The prior of both estimators are $N(0,\\frac{1}{10^{-6}})$\n",
    "\n",
    "<img src=\"figures/example_bayesian.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Estimation: Bayesian with (wrongly) informed prior\n",
    "\n",
    "The prior of the constant term is set to 50\n",
    "\n",
    "<img src=\"figures/example_all_methods.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Bayesian Statistics: A Brief History\n",
    "\n",
    "<img src=\"figures/thomas_bayes.png\" width=\"250\">\n",
    "\n",
    "- Thomas Bayes (1702 - 1761): a pastor, also trained as a statistician and logician. \n",
    "- A special case of (what we now know as) the Bayes Theorem was formulated by him in unpublished work\n",
    " - Laplace generalized the formulation afterwards, but kept Bayes' name attached to it\n",
    "   - Laplace had already attached his name to a lot of things by then\n",
    "\n",
    "The idea behind the theorem was to construct conditional probabilities using:\n",
    "- One's perceptions, before the evidence is analyzed\n",
    "- Analyzing the evidence and then updating these perceptions\n",
    "\n",
    "More history and intuitive explanations can be found [here!](https://www.lesswrong.com/posts/RTt59BtFLqQbsSiqd/a-history-of-bayes-theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian vs Frequentist Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Frequentist Approach\n",
    "\n",
    "Ingredients:\n",
    "- model with parameter vector $\\theta$\n",
    "- data $y$ (randomly sampled from an infinite population)\n",
    "\n",
    "Approach\n",
    "- There is a true $\\theta_0$ that generated the data\n",
    "- Estimators of $\\theta_0$ are constructed by maximizing the likelihood we observe this particular draw of $y$ \n",
    " - Resulting test statistics (e.g. confidence intervals, standard errors) are constructed to explain the population where $y$ was sampled from\n",
    " - This puts a lot of pressure on the draw of $y$ we observe (e.g. randomness, sufficient size etc.) \n",
    "- Main Inference: a point estimator, $\\hat{\\theta_0}$. \n",
    " - **Important**: inference is contingent on model assumptions. Can you think of one assumption we've seen so far today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian Approach\n",
    "\n",
    "Ingredients:\n",
    "- model with parameter vector $\\theta$\n",
    "- data $y$ (no assumptions)\n",
    "\n",
    "Approach\n",
    "- Parameters are treated as random variables\n",
    " - Our belief about the distribution of $\\theta_0$, before seeing the data, is the \"prior\"\n",
    "- This prior belief is updated when the data is observed\n",
    " - The result of this updating is the posterior distribution of $\\theta_0$\n",
    "- Main Inference: the posterior distribution of $\\hat{\\theta_0}$. From this, one can get:\n",
    " - Mean, median, mode $\\hat{\\theta_0}$\n",
    " - Standard deviation, credibility interval (CI) of $\\hat{\\theta_0}$\n",
    " - ... and anything else you can think of extracting from a series of numbers!\n",
    "- **Important**: inference is not contingent on $y$. But it is heavily influenced by the prior beliefs\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Statistics: Formula\n",
    "\n",
    "Ingredients\n",
    "- A statistical model, with a likelihood function $\\rightarrow$ $p(y|\\theta)$\n",
    "- Prior belief (including not having a clue) about the parameter $\\rightarrow$ $p(\\theta)$\n",
    "\n",
    "Aim is to get the posterior distribution, $P(\\theta|y)$. This is done via integration, with respect to $\\theta$, of the Bayes theorem:\n",
    "\n",
    "$$ p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} $$\n",
    "\n",
    "The denominator, $p(y)$ is not part of our ingredients but is in the theorem! \n",
    " - After integration, $p(y)$ comes out to be a constant independent of $\\theta$ $\\Rightarrow p(\\theta|y) \\propto p(y|\\theta)p(\\theta) $\n",
    "\n",
    "We use the above formulation to calculate marginal posterior densities for all elements in $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Bayesian OLS\n",
    "\n",
    "Model: $ y = X\\beta + \\epsilon $, with $ \\epsilon \\sim N(0, \\sigma^2) $\n",
    "\n",
    "Ingredients:\n",
    "- Model: $p(y|\\beta,\\sigma) = \\left(\\frac{1}{\\sigma\\sqrt(2\\pi)}\\right)^N exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\beta)'(y - X\\beta) \\right)$ \n",
    "- Diffuse priors: $p(\\beta) \\propto 1$ and $p(ln(\\sigma^2)) \\propto 1$\n",
    "\n",
    "Joint-density of posterior:\n",
    "\n",
    "$$p(\\beta, \\sigma | y) = \\left(\\frac{1}{\\sigma}\\right)^{N + 2}exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\beta)'(y - X\\beta)\\right) $$\n",
    "\n",
    "Marginal densities of $\\beta$ and $\\sigma$:\n",
    "$$p(\\beta| y) \\propto \\int_0^{\\infty} \\left(\\frac{1}{\\sigma}\\right)^{N + 2}exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\beta)'(y - X\\beta)\\right) d\\sigma $$\n",
    "\n",
    "$$p(\\sigma| y) \\propto \\int_{-\\infty}^{\\infty} \\left(\\frac{1}{\\sigma}\\right)^{N + 2}exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\beta)'(y - X\\beta)\\right) d\\beta $$\n",
    "\n",
    "\n",
    "Now we are going to solve these integrals by hand!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Integral by Hand\n",
    "\n",
    "<img src=\"figures/solve_integral.jpg\" width=\"450\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Work Around The Integral -  Sampling\n",
    "\n",
    "The marginal posterior densities are not always straightforward, or even possible, to calculate by hand. Sampling algorithms help us by sampling from marginal posterior densities\n",
    "\n",
    "- Each parameter ($\\beta$ and $\\sigma$) is sampled, conditional on the other parameters\n",
    "- Recall: the space is defined by the prior. So when we set the prior, we end up setting the interval of possible values for $\\beta$ and $\\sigma$  \n",
    "\n",
    "Multiple techniques exist, but Monte Carlo Markov Chain methods (MCMC, see [here](http://faculty.econ.ucdavis.edu/faculty/jorda/bayes/mcmc.pdf) to dig deeper) are most used. Three popular MCMC techniques are:\n",
    "- Gibbs Sampler\n",
    "- Metropolis-Hastings Sampler\n",
    "- Data Augmentation\n",
    "\n",
    "The output of sampling is a Markov chain. Some things to keep in mind:\n",
    "- Due to sampling procedures, the values in the chain are correlated to each other, so thinning (selecting every $k^{th}$ observation) is often used\n",
    "- Check for convergence, i.e. that the resulting Markov chain is indeed from the marginal posterior density\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convergence: An Art\n",
    "\n",
    "In principle, it is not possible to directly test if convergence is achieved or not. Instead, here is what is done:\n",
    "1. Run multiple (2, maybe 3) samplings in parallel. These are known as *chains*\n",
    "2. At the end of your sampling, compare these chains:\n",
    " * Plot them, see that they overlap sufficiently\n",
    " * Run some hypothesis tests to see if they are different or not\n",
    "3. If you think that convergence has not been achieved, then re-run the simulations:\n",
    " * Add more chains\n",
    " * Increase burn-in sample\n",
    " * Change sampling algorithm\n",
    "4. Repeat steps 2 and 3, until you are convinced convergence has been achieved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Bayesian Estimator\n",
    "\n",
    "Congratulations! You have a Bayesian estimator! This is a series of numbers, that you can:\n",
    "* Calculate the mean of\n",
    "* Calculate the standard deviation of\n",
    "* See if the middle 95% of the numbers include 0\n",
    "* See how much overlap there are between different parameters\n",
    "\n",
    "It also holds information about your prior:\n",
    "* If your prior was flat (i.e. you didn't know much about your data) $\\rightarrow$ the Bayesian estimator is relatively wide\n",
    "* If your prior was informative (i.e. you have some specialist knowledge) $\\rightarrow$ the Bayesian estimator could be relatively narrow\n",
    "\n",
    "So we are able to systematically include what we know and don't know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Dis)Advantages of the Bayesian Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Advantages\n",
    "\n",
    "* Intuitive result\n",
    "* Factors in prior knowledge in a transparent way\n",
    "* Makes no assumptions on the dataset\n",
    "* The posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Disadvantages\n",
    "\n",
    "* Convergence\n",
    "* Not as well-known as frequentist methods\n",
    "* Requires much more computing power\n",
    "* There is no rule of thumb of what prior you should use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take-Away\n",
    "\n",
    "This is an alternative way of approaching statistical inference\n",
    "\n",
    "* It can be useful for some cases\n",
    "* It can be unnecessary for other cases\n",
    "\n",
    "But here is how I like to imagine some benefits, within ING\n",
    "\n",
    "* Setting priors $\\rightarrow$ letting the business side put their specialist knowledge to good use\n",
    " * \"Well I know that mortgage lending is negatively influenced by interest rates\" (i.e. prior parameter is in the negative range)\n",
    "* Bayesian estimator $\\rightarrow$ giving probabilities of different outcomes to the business side\n",
    " * \"60% of our forecasts are showing a profit, but 40% show that we will make a loss\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You!\n",
    "\n",
    "<img src=\"figures/xkcd_bayes.png\" width=\"350\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
