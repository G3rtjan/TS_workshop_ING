{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Time-varying AR(1) exercise\n",
    "\n",
    "> Joris de Wind (1:1 Analytics, DBNL)\n",
    "\n",
    "### Model description\n",
    "\n",
    "In this exercise, you will learn about autoregressive models with time-varying parameters. In particular, we will use a Bayesian approach to estimate the time-varying AR(1) model\n",
    "\n",
    "> $y_t = \\mu_t + \\varrho_t y_{t-1} + \\varepsilon_t$\n",
    "with $\\varepsilon_t \\sim N(0, R)$\n",
    "\n",
    "where the coefficients are assumed to follow a random walk, that is\n",
    "\n",
    "> $\\theta_t = [\\mu_t, \\varrho_t]' = \\theta_{t-1} + \\upsilon_t$\n",
    "with $\\upsilon_t \\sim N(0, Q)$\n",
    "\n",
    "The covariance matrix $Q$ is very important, as it controls the amount of time variation in the parameters. The larger $Q$ the larger the variance of the changes in the time-varying parameters.\n",
    "\n",
    "### Example and some explanation\n",
    "\n",
    "This model (and in particular its multivariate generalizations) can for example be used to model inflation. The persistence and target level of inflation can be influenced by changes in monetary policy, which will be reflected in changes in $\\varrho_t$ and $\\mu_t$, respectively. As a matter of fact, if the central bank becomes more effective the persistence of the inflation process will drop, i.e. the economy will recover faster after a shock to the economy.\n",
    "\n",
    "Since the data can be explained both by temporary shocks $\\varepsilon_t$ as well as permanent structural changes to the system $\\upsilon_t$, we need to let the data speak which of the two explanations is more realistic. The most likely estimate is typically a combination of the two, but the weights depend on the circumstances and it's a _dynamic_ process. For example, a lower inflation rate might at first be explained by temporary shocks but if inflation continues to be low for a longer period it will be a good idea to revise this assessment and consider the possibility that the central bank actually lowered its target inflation rate.\n",
    "\n",
    "### Estimation procedure\n",
    "\n",
    "To estimate this model we will use the __Gibbs sampler__ as well as the __Kalman filter__. But don't worry: the goal of this exercise is to teach you about the structure of the __Bayesian estimation__ procedure rather than the tiny details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import invwishart\n",
    "from scipy.linalg import solve, cholesky\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: generating coefficients and fake data\n",
    "\n",
    "For illustration purposes, we will work with fake data in this exercise. This enables us to compare the estimated trajectory for the time-varying parameters with the true trajectory with which the data was generated.\n",
    "\n",
    "You can just run this part of the code, i.e. there are no exercises in part 1. Of course, you are more than welcome to come back to this part after you have successfully finished all the exercises in the remainder of this notebook.\n",
    "\n",
    "In particular, it would be interesting to _experiment_ with a different trajectory for the parameters, and study the consequences on the accuracy of the estimated trajectory. For example, you could study the consequences when the parameter change is not gradual (as in the code below) but instead very abrupt. Or, you could estimate the model on real data!\n",
    "\n",
    "**Okay, one non-programming exercise after all**: can you explain the connection between the two plots generated below? In particular, do you understand the effects of the parameter changes on the simulated data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating evolution of coefficients\n",
    "number_of_periods = 240\n",
    "rho = np.zeros(number_of_periods)\n",
    "rho[0:80] = 0.9\n",
    "rho[80:130] = 0.9 + (0.5 - 0.9) * 1 / (1 + np.exp(-0.25 * np.arange(-25, 25)))\n",
    "rho[130:240] = 0.5\n",
    "const = 1 - rho  # to keep the unconditional mean constant\n",
    "sig = 0.007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(rho, color='r', linewidth=3, label=r\"$\\rho$\")\n",
    "plt.plot(const, color='b', linewidth=3, label=\"constant\")\n",
    "plt.title(\"Evolution of coefficients\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"t\", style='italic')\n",
    "plt.legend(fontsize=14)\n",
    "txt = \\\n",
    "\"Back to the central banking example,\\n\" + \\\n",
    "\"around period 100 a very smart data\\n\" + \\\n",
    "\"scientist is hired by the central bank.\\n\\n\" + \\\n",
    "\"This data scientist can build models to\\n\" + \\\n",
    "\"design more effective monetary policy,\\n\" + \\\n",
    "\"making the economy more resilient to\\n\" + \\\n",
    "\"shocks and therefore decreasing the\\n\" + \\\n",
    "\"persistence of inflation.\\n\\n\" + \\\n",
    "\"Some smart market watchers were able\\n\" + \\\n",
    "\"to anticipate the drop in persistence,\\n\" + \\\n",
    "\"which led to a drop in persistence even\\n\" + \\\n",
    "\"before the actual increase in policy\\n\" + \\\n",
    "\"effectiveness! Of course, there are\\n\" + \\\n",
    "\"always people with a delayed\\nresponse.\"\n",
    "plt.text(-2, 0.25, txt, fontsize=12.8, style='italic')\n",
    "plt.axvline(x=100, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating fake data\n",
    "np.random.seed(seed=179)\n",
    "data = np.zeros(number_of_periods)\n",
    "data[0] = const[0] / (1 - rho[0])  # start at the unconditional mean\n",
    "for i in range(1, number_of_periods):\n",
    "    data[i] = const[i] + rho[i] * data[i-1] + sig * np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(data, color='k', linewidth=3)\n",
    "plt.title(\"Fake data\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"t\", style='italic')\n",
    "txt = \"Can you guess in which part of the sample\\nperiod the data was more persistent?\"\n",
    "plt.text(80, 1.03, txt, fontsize='14', style='italic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: preparing the data and choosing priors\n",
    "\n",
    "Before running the Bayesian estimation procedure, we need to decide which priors to use. One of the standard approaches is to use a training sample, and use time-invariant OLS on the training sample to calibrate the priors. Of course, it's not allowed to recycle the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the data\n",
    "p = 1\n",
    "T = len(data) - p\n",
    "k = 1 + p  # number of time-varying parameters (constant + autoregressive)\n",
    "y = data[p:]\n",
    "X = np.ones((T, 2))\n",
    "X[:, 1] = data[:-p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - splitting the data\n",
    "\n",
    "Split the data, i.e. take the first 40 observations as training sample and the remainder as the sample that's going to be used in the estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data (we use a training sample to calibrate the prior)\n",
    "yp = XXX  # for training sample\n",
    "Xp = XXX  # for training sample\n",
    "y = XXX   # for regular sample\n",
    "X = XXX   # for regular sample\n",
    "T = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS on training sample\n",
    "ols = sm.OLS(yp, Xp).fit()  # time-invariant OLS on training sample\n",
    "                            # to calibrate the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - setting the priors\n",
    "\n",
    "We need to come up with priors for\n",
    "\n",
    "- $\\theta_0$\n",
    "- $Q$\n",
    "- $R$\n",
    "\n",
    "The prior for $R$ is already taken care of, free of charge! So, let's focus on the priors for $\\theta_0$ and $Q$. Note that the prior for $\\theta_0$ together with the prior for $Q$ automatically imply a prior for the entire trajectory of $\\theta_t$ (through the random walk equation). **Make sure you understand this, and otherwise ask your neighbor!**\n",
    "\n",
    "We will use the OLS estimates on the training sample to calibrate the priors for both $\\theta_0$ and $Q$. Based on the training sample, we have a rough idea for the appropriate level of $\\theta_0$ and we can use the OLS estimation uncertainty (i.e. the covariance matrix of the parameter estimation errors) as a basis for the prior on the amount of time variation, which is parameterized by $Q$.\n",
    "\n",
    "Anyway, regarding $\\theta_0$ we want to use a normally distributed prior with\n",
    "\n",
    "- mean equal to the time-invariant OLS estimate on the training sample\n",
    "- variance proportional to the covariance matrix of the parameter estimation errors\n",
    "\n",
    "and regarding $Q$ we want to use an inverse-Wishart distribution with$^1$\n",
    "\n",
    "- scale matrix proportional to the covariance matrix of the parameter estimation errors\n",
    "- degrees-of-freedom set to k+1, i.e. the number of time-varying parameters plus one\n",
    "\n",
    "$^1$Don't worry if you have never heard about the inverse-Wishart distribution, it's a multivariate generalization of the inverse-Gamma distribution (with some additional restrictions), and it's used quite often in Bayesian statistics. In any case, the inverse-Wishart distribution is parameterized by a scale matrix and degrees-of-freedom parameter. The mean and mode of the inverse-Wishart distribution are proportional to the scale matrix and the variances of its elements are decreasing in the degrees-of-freedom parameter.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Set `THbar` and `PeK[:, :, 0]` to the above discussed prior mean and variance for $\\theta_0$, respectively. Note that PeK is a 3D-array and the rest of this array is filled with zeros. Don't worry about that, that's just for memory allocation.\n",
    "\n",
    "2. Set `Qbar` and `T0Q` to the above discussed scale matrix and degrees-of-freedom for $Q$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior on the time-varying parameters\n",
    "THbar = XXX\n",
    "PeK = np.zeros((k, k, T+1))\n",
    "PeK[:, :, 0] = 4 * XXX\n",
    "\n",
    "# prior on the shocks to the parameters\n",
    "Qbar = 0.1 * XXX\n",
    "T0Q = XXX\n",
    "\n",
    "# prior on the covariance matrix\n",
    "Rbar = ols.mse_resid\n",
    "T0R = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Bayesian estimation using Gibbs sampling\n",
    "\n",
    "Here is where the magic happens. We are interested in estimating the joint posterior density$^1$\n",
    "\n",
    "> $P(\\theta^T, Q, R|y^T)$\n",
    "\n",
    "but unfortunately we don't have an analytical expression for this density. This is where the Gibbs sampler comes to the rescue.\n",
    "\n",
    "With the Gibbs sampler we basically iterate between the various conditional posterior densities, and under some regularity conditions (which are satisfied in this case) this process will converge to the joint posterior density.\n",
    "\n",
    "In the current case, the Gibbs sampler is very useful because we can derive analytical expressions for the various conditional posterior densities:\n",
    "\n",
    "- $P(\\theta^T|Q, R, y^T)$\n",
    "- $P(Q|\\theta^T, R, y^T)$\n",
    "- $P(R|\\theta^T, Q, y^T)$\n",
    "\n",
    "Regarding $P(\\theta^T|Q, R, y^T)$, for given covariance matrices $Q$ and $R$ we simply have a fully parameterized state space model. And we therefore need the Kalman filter ... Well, the Kalman filter yields point estimates, and we actually need to run the Kalman simulation smoother to draw random samples. You don't need to program this step yourself, i.e. the simulation smoother function will be provided below (we did the hard work for you!).\n",
    "\n",
    "Regarding $P(Q|\\theta^T, R, y^T)$, for given $\\theta^T$ we can calculate the realized shocks to the time-varying parameters (i.e. take the first difference of $\\theta_t$), from which we can calculate the sum-of-squared residuals. The sum-of-squared residuals form the basis to simulate a random covariance matrix using the inverse-Wishart distribution.\n",
    "\n",
    "Regarding $P(R|\\theta^T, Q, y^T)$, for given $\\theta^T$ we can calculate the realized shocks in the AR(1) equation (i.e. simply use $\\varepsilon_t = y_t - \\mu_t - \\varrho_t y_{t-1}$), from which we can calculate the sum-of-squared residuals. The sum-of-squared residuals form the basis to simulate a random covariance matrix using the inverse-Wishart distribution.\n",
    "\n",
    "$^1$With superscript $x^T$ we mean to include the entire trajectory of $x_t$ from $x_0$ up to $x_T$ inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simulation smoother function (needed in Gibbs sampler below)\n",
    "@jit\n",
    "def sampleTH(Q, R, Pe, TH):\n",
    "    # Kalman filter\n",
    "    Po = np.zeros((k, k, T))  # memory allocation\n",
    "    for t in range(T):\n",
    "        Xtemp = X[t, :].reshape(1, -1)\n",
    "        Po[:, :, t] = Pe[:, :, t] + Q\n",
    "        K = solve(Xtemp @ Po[:, :, t] @ Xtemp.T + R,   # note that solve is numerically more\n",
    "                  Xtemp @ Po[:, :, t]).reshape(-1, 1)  # stable than directly using inv\n",
    "        Pe[:, :, t+1] = Po[:, :, t] - K @ Xtemp @ Po[:, :, t]\n",
    "        TH[:, t+1] = TH[:, t] + K @ (y[t] - Xtemp @ TH[:, t])\n",
    "    # Kalman simulation smoother\n",
    "    TH[:, T] = TH[:, T] + cholesky(Pe[:, :, T]).T @ \\\n",
    "               np.random.randn(k)\n",
    "    for t in reversed(range(T)):\n",
    "        temp1 = Pe[:, :, t]\n",
    "        temp2 = solve(Po[:, :, t], temp1).T\n",
    "        TH[:, t] = TH[:, t] + temp2 @ (TH[:, t+1] - TH[:, t]) + \\\n",
    "                   cholesky(temp1 - temp2 @ temp1).T @ \\\n",
    "                   np.random.randn(k)\n",
    "    return TH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - complete the Gibbs sampler\n",
    "\n",
    "You will basically need to complete the for loop below.\n",
    "\n",
    "1. To draw a new trajectory for $\\theta^T$, you will need to call the `sampleTH` function with the most recent draws for $Q$ and $R$. Note that you will also need to provide the arguments `PeK` and `TH[:, :, i]`.\n",
    "2. To draw a new covariance matrix $Q$, you will need to calculate the realized shocks to the time-varying parameters (`v` in the code below) based on the most recent trajectory for $\\theta^T$. After calculating `v`, you can calculate the sum-of-squared residuals, which will in turn be used in the `invwishart.rvs` sampling function.\n",
    "3. To draw a new covariance matrix $R$, you will need to calculate the realized shocks in the AR(1) equation (`u` in the code below). After this, you can calculate the sum-of-squared residuals, which will in turn be used in the `invwishart.rvs` sampling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian estimation\n",
    "g = 500  # length of Markov chain\n",
    "b = 0.2  # burn-in fraction\n",
    "\n",
    "# allocating memory and initialization\n",
    "TH = np.zeros((k, T+1, g))\n",
    "Q1 = np.zeros((k, k, g+1))\n",
    "R1 = np.zeros((1, 1, g+1))\n",
    "Q1[:, :, 0] = Qbar\n",
    "R1[:, :, 0] = Rbar\n",
    "\n",
    "# Gibbs sampler\n",
    "for i in range(g):\n",
    "\n",
    "    # sample TH (given Q1 and R1)\n",
    "    TH[:, 0, i] = THbar\n",
    "    TH[:, :, i] = XXX\n",
    "\n",
    "    # sample Q1 (given TH and R1)\n",
    "    v = XXX\n",
    "    SSRv = XXX\n",
    "    Qtemp = SSRv + T0Q * Qbar\n",
    "    Qtemp = (Qtemp + Qtemp.T) / 2  # to improve numerical stability\n",
    "    Q1[:, :, i+1] = invwishart.rvs(T + T0Q, Qtemp)\n",
    "\n",
    "    # sample R1 (given TH and Q1)\n",
    "    u = XXX\n",
    "    SSRu = XXX\n",
    "    Rtemp = SSRu + T0R * Rbar\n",
    "    Rtemp = (Rtemp + Rtemp.T) / 2  # to improve numerical stability\n",
    "    R1[:, :, i+1] = invwishart.rvs(T + T0R, Rtemp)\n",
    "\n",
    "    # check progress\n",
    "    if (i + 1) % (g / 10) == 0:\n",
    "        print(f\"Progress report: {100 * (i + 1) / g}% done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 4: plotting the results\n",
    "\n",
    "After finishing all the exercises above you can just run the code below and see how well the estimated trajectory tracks the true one. Of course, there is still quite some sampling uncertainty left but you did a good job, didn't you?\n",
    "\n",
    "Don't celebrate too early, there are still several extra exercises below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH_estimated = np.median(TH[:, :, int(b*g):], axis=2)  # discard burn-in samples\n",
    "TH_estimated = np.concatenate((np.tile(ols.params, (int(ols.nobs), 1)).T,\n",
    "                              TH_estimated), axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(rho, color='r', linewidth=3, label=\"true value\")\n",
    "plt.plot(TH_estimated[1, :], color='b', linewidth=3, label=\"estimated\")\n",
    "plt.title(r\"Evolution of $\\rho$\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"t\", style='italic')\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra exercises - you need to finish at least one before you get the password for free drinks!\n",
    "\n",
    "Feel free to pick one or two exercises that you find most interesting.\n",
    "\n",
    "1. Adjust the code so as to estimate a time-varying AR(2) model. Tip: you only need to change the code of a single cell!\n",
    "2. Play around with the constant-of-proportionality in the prior scale matrix for $Q$, and study the consequences on the estimated amount of time variation\n",
    "3. Go back to part 1 and adjust the underlying trajectory of the parameters, and study the consequences on the accuracy of the estimated trajectory\n",
    "4. Adjust the code to be able to run multiple Markov chains (i.e. multiple instances of the Gibbs sampler), and compare the chains to check the convergence of the Bayesian estimation procedure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
