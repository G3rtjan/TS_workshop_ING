{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN and LSTM Beginners version\n",
    "\n",
    "**Audience:**\n",
    "This version of the workshop is dedicated to participants with limited knowledge about neural networks, Keras and Python.\n",
    "\n",
    "**Purpose:**\n",
    "Provide participants with some examples of how to use neural network for time series problem. Participant is encouraged to experiment with architectures to see how that influences the results.\n",
    "\n",
    "The following helper functions are provided:\n",
    "* load_data - load the data from the csv file\n",
    "* create_dataset - creates two numpy arrays with target and features. Features are lagged prices of the stock\n",
    "* out_of_time - split in train and test\n",
    "\n",
    "Overview of the tutorial:\n",
    "\n",
    "* Data Praparetion:\n",
    "  - load the data \n",
    "  - explore the target\n",
    "  - convert data to numpy array\n",
    "  - make features: lagged price\n",
    "  - split the data intro train and test \n",
    "  - normalize the data\n",
    "\n",
    "* Feed Forward Neural Network:\n",
    "    - Define the neural network input shape\n",
    "    - Define the number of neurons, layers and activation functions\n",
    "    - Define the output of the neural network\n",
    "    - Define the loss of the neural network\n",
    "    - Compile the neural network\n",
    "    - Train the neural network\n",
    "    - Keep on eye on the loss, is the model training?\n",
    "    - Plot the predictions again the real data\n",
    "    - Redo the above with different paraments and model architectures\n",
    "    \n",
    "* LSTM:\n",
    "    - Define the neural network input shape. Reminder: LSTM needs a special shape\n",
    "    - Define the number of neurons, layers and activation functions\n",
    "    - Define the output of the neural network\n",
    "    - Define the loss of the neural network\n",
    "    - Compile the neural network\n",
    "    - Train the neural network\n",
    "    - Keep on eye on the loss, is the model training?\n",
    "    - Plot the predictions again the real data\n",
    "    - Redo the above with different paraments and model architectures\n",
    "    \n",
    "    \n",
    "Neural Networks Cheat Sheet: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(symbol = 'SP500'):\n",
    "    data = pd.read_csv('data_stocks.csv')\n",
    "    data = data[['DATE',symbol]].sort_values(by=['DATE'])\n",
    "    return data\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def out_of_time(dataset, train_size):\n",
    "    train_size = int(len(dataset) * train_size)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    return train, test\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward NN - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data, by default we use S&P500\n",
    "stock_data = load_data()\n",
    "\n",
    "# visualising the data\n",
    "stock_data['SP500'].plot(figsize = (15,10))\n",
    "\n",
    "# connverting data to numoy arrays. \n",
    "stock_data = stock_data['SP500'].values.reshape(stock_data.shape[0],1)\n",
    "\n",
    "# normalizing the data. Remember, neural nets work better with normalized data\n",
    "stock_data = scaler.fit_transform(stock_data)\n",
    "\n",
    "# Split by trian and test. First 85% of the data for training and the rest is for\n",
    "# testing\n",
    "train, test = out_of_time(stock_data,0.85)\n",
    "\n",
    "# using lagged price of the stock as feature. Here look back is number of\n",
    "# previous prices to take as a feature\n",
    "X_train, Y_train = create_dataset(train, look_back=5)\n",
    "X_test, Y_test = create_dataset(test, look_back=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward NN - Bulding the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the model.\n",
    "\n",
    "We will do a regular feedworward model, so we select model type Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the first hidden layer in the neural network.\n",
    "We specify the shape of the input to the hidder layer and the shape of the hidden layer.\n",
    "\n",
    "We have 5 features, thus shape of our input to the naural network is 5.\n",
    "We want 10 neurons in the hidder layer, so we specify dinemtion of 10.\n",
    "In addition we apply a relu activation funtion to the output of first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10, input_dim=5, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a second layer we simply use add method of the model here we only need to speciy the dimentions and activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(20, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to specify the output layer of the neural network. since it is a regression problem, out output is one number and shape is 1. Also, no activation function is spplied, by using 'linear' option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our architecture figured out, we need to define loss and which algorythim to use for training the model weights. Since it is a regression problem we use mean square error MSE, with adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply need to fit the model on the data. Here we need to specify the number of epochs, that is the number of time neural network will go through the dataset to train\n",
    "\n",
    "While model is training, keep an eye on the loss, is it decreasing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,Y_train,epochs=4,validation_data=(X_test,Y_test),shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward NN - Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will plot the prediction of the model versus actual data to see how well it trained\n",
    "\n",
    "# First we need to tranform the data back to original scare\n",
    "prediction_nn = scaler.inverse_transform(model.predict(X_test))\n",
    "\n",
    "# Then we need to reshape the data into the correct dimention\n",
    "prediction_nn = prediction_nn.reshape(prediction_nn.shape[0], )\n",
    "\n",
    "# Also we need to transform the target to the ofiginal scale\n",
    "true_data = scaler.inverse_transform([Y_test])\n",
    "\n",
    "# And now we can plot the results\n",
    "plt.plot(prediction_nn)\n",
    "plt.plot(true_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network - Self Activities\n",
    "\n",
    " * change the number of features\n",
    " * add more layers/neurons\n",
    " * try different activation functions\n",
    " * was scaling done correct? we scaled before splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data, by default we use S&P500\n",
    "stock_data = load_data()\n",
    "\n",
    "# visualising the data\n",
    "stock_data['SP500'].plot(figsize = (15,10))\n",
    "\n",
    "# connverting data to numoy arrays. \n",
    "stock_data = stock_data['SP500'].values.reshape(stock_data.shape[0],1)\n",
    "\n",
    "# normalizing the data. Remember, neural nets work better with normalized data\n",
    "stock_data = scaler.fit_transform(stock_data)\n",
    "\n",
    "# Split by trian and test. First 85% of the data for training and the rest is for\n",
    "# testing\n",
    "train, test = out_of_time(stock_data,0.85)\n",
    "\n",
    "# using lagged price of the stock as feature. Here look back is number of\n",
    "# previous prices to take as a feature\n",
    "X_train, Y_train = create_dataset(train, look_back=5)\n",
    "X_test, Y_test = create_dataset(test, look_back=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the steps of the data preparation are exactly the same as in the previous part. However, LSTMs require data to be in specific shape. \n",
    "\n",
    "LSTMs require data to be a 3D array with following elements:\n",
    " * length training data -> N\n",
    " * length of sequance we want to predict -> Y_l\n",
    " * number of features -> X_l\n",
    " \n",
    "So the 3D array takes the following form: (N, Y_l, X_l)\n",
    "\n",
    "In our case, the length of the training data is 35070, we are predicting sequance of 1 (next price) and we have 5 features. \n",
    "\n",
    "Thus our train data has shape: (35070, 1 , 5). Same is done for test data.\n",
    "\n",
    "I do advise to look into the final data and see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Bulding the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we first initiate a model type. Here it is again sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add an LSTM cell to our model. We need to specify model input shape and number of neurons on the LSMT cell. \n",
    "\n",
    "We will be feeding one observat at a time with 5 features, so our input shape is: (1,5)\n",
    "\n",
    "LSTM sells are rather standardised in terms of activations functions and achitecture inside the cell. We can change number of neurons in the cell. That is done by specifying 10 as shape for the cell. \n",
    "\n",
    "In princical you can shange the entire structure of the LSTM cell, like activation functions and such, but for that you need to use Tensor Flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(10, input_shape = (1, X_train[0].shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to specify the output layer of the neural network. since it is a regression problem, out output is one number and shape is 1. Also, no activation function is spplied, by using 'linear' option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our architecture figured out, we need to define loss and which algorythim to use for training the model weights. Since it is a regression problem we use mean square error MSE, with adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply need to fit the model on the data. Here we need to specify the number of epochs, that is the number of time neural network will go through the dataset to train. Keep in mind that LSTMs have a lot of parameters, so in genral they need a lot epochs to train properly.\n",
    "\n",
    "While model is training, keep an eye on the loss, is it decreasing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,Y_train,epochs=15,validation_data=(X_test,Y_test),shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will plot the prediction of the model versus actual data to see how well it trained\n",
    "\n",
    "# First we need to tranform the data back to original scare\n",
    "prediction_lstm = scaler.inverse_transform(model.predict(X_test))\n",
    "\n",
    "# Then we need to reshape the data into the correct dimention\n",
    "prediction_lstm = prediction_lstm.reshape(prediction_lstm.shape[0], )\n",
    "\n",
    "# And now we can plot the results\n",
    "plt.plot(prediction_nn)\n",
    "plt.plot(prediction_lstm)\n",
    "true_data = scaler.inverse_transform([Y_test])\n",
    "plt.plot(true_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Self Activities\n",
    "\n",
    " * change the number of features\n",
    " * add more neurons to LSTM\n",
    " * maybe try adding other layer after LSTM to see what you get\n",
    " * was scaling done correct? we scaled before splitting the data\n",
    " * was the results of LSTM bettwe or worse than NN? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
